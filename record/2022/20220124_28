////////////// 20220124 //////////////
  数据块分配和检索方式的改变属于是功能优化问题，可以暂时先不去考虑，等基本功能完善之后再去处理

  目前每个文件既要分配一个 vnode，又要创建一个 VFile 类，其实是有点冗余的。vfs 动态分配 vnode 的行为其实
就类似于我们 new 一个新的文件类对象，可能就是在开销上会有一点差异。所以，下一步文件系统设计的目标应该是将 vnode 
逐渐融合到 VFile 类当中，所有文件都只对应一个 VFile class，而不用再去动态分配 vnode


////////////// 20220125 //////////////
  块组分配的话还是以数据块位图作为分割标准，然后根据数据块位图再去切割 inode 位图，尽量保证两者都是工整的。
SV39 = 2^32(4GB) * 2^7(128) = 512G，留给持久化内存的地址空间大概是 128G；

  一个数据块包含 4096 * 8 = 32768(2^15) bits，可管理的内存空间为 32768 * 4k = 128M，所以我们就需要大约
1024 个内存页作为数据块位图来管理 128G 的空间

  超级块中除去已有的基本参数字段之外，剩余空间大概可以装下130多个 struct group，这样 tptfs 就可以分成 128 个分区，
每个分区数据块位图占 4 个内存页

  每个分区占有的数据页 = 4*4096*8，假设平均每个 inode 对应 4 个数据页，那一个分区则包含有 4096*8 = 32768 个 inode，
这样每个分区中的 inode 位图也刚好占有一个内存页

  上面说的 128 个分区不应该是定死的，而应该是根据实际计算出的数值进行调整。最优先满足的应该是每个块组中占用的内存页的数量应该
是4的整数倍，并且把 128 作为块组数量的一个计算标准，并且是最大值。就以上述为例，由于并不是所有的数据块都是存放文件数据的，所以
块位图肯定是占不满 1024 个页，平均分配每个组不到4个页，这时候就可以以4作为每个分组的数据块位图页个数。(分配文件系统持久化内存
空间一般都是128、256 这种，所以位图个数也差不多就是4、8)。
  确定了每个组数据块位图页个数之后，还要再根据实际管理的页个数，计算一共需要分配多少个页的块位图，因为很可能用不了 128 个组，
说不定 127 个就够了，那此时就分配 127 个，最后一个停用，然后需要在超级块中添加一个字段，用于记录总共分配了多少个块组


////////////// 20220126 //////////////
  ext2 中的块组数量是可以由用于指定的，这说明块组的分配并不是完全通过磁盘的硬件特性去。而且固态硬盘应该就没有磁头，柱面这种概念了，
但还是要格式化成某种类型的文件系统，说明块组这个概念跟磁盘硬件结构并不是强关联。感觉像是软件层面的概念，作用就是更好的分配和管理数据块


////////////// 20220127 //////////////
  文件系统中的数据区的排布也要尽可能工整，这样数据访问起来才会比较方便，并且不容易出错。目前的分组还是以 block bitmap 作为基准，
这时我们可以保证的是每个分组的 bitmap block、data block 是工整的(因为都是整块分配)。因为每个 page 中保存的 inode 结构体数量
是一定的(目前是16个)，所以我们也比较容易保证 inode table block 是工整的(最起码能保证都是整数块，不一定是 power of 2)，但是
却不能保证这些 inode 在 inode bitmap block 中也是完整排布的，中间大概率会产生空缺

  此时我们如果要分配一个新的 inode 的时候，可能就要遍历整个位图。如果起始位置是在某个块中间的话，那处理起来就会比较麻烦，因为需要
考虑跨数据块的问题。所以，目前打算是将 inode bitmap 进行拆分。现在的话是整个一起处理，其实可以改成每个 group 单独对应一个 inode
bitmap 区。这样做的好处就是每个 group 对应的都是独立的部分，互不影响。坏处就是可能会损失一部分空间，因为不可能每次都刚好占用完整的
数据块，但是这种损失我个人认为是完全可以接受的


////////////// 20220128 //////////////
  今天发现了一个问题，比如说线程A要为一个文件申请一个数据块，然后它去读取 bitmap，数据读到了，假设100编号的块可以使用。然后该线程
被线程B抢占了，B也要申请一个数据块，它也去读bitmap，发现100能用，它抢先把这个数据块用了。然后切换到A，此时A仍然认为100可用，结果
往里边写数据。这不是相当于两个文件共用一个数据块了？
  ext2 中的方式处理是通过 bread 这个函数，但是tptfs是直接指针指向数据地址，是不是会存在同步问题啊？

  如果需要加锁的话，我们要在尽可能小的粒度上去锁一块资源。加入我们要对一个文件进行操作的时候，我们不应该去把整个目录项给锁上，而是仅仅
锁住这个文件。同理，当我们需要给某个文件分配磁盘块的时候，最好的做法就是锁住这一个块组的 bitmap 区域，而不是锁住整个文件系统的 bitmap。
这样访问其他块组的线程就不会因为获取不到资源而被迫休眠。
  这样又会带来另外一个问题，就是每个块组都要对应一个锁，进而管理本块组中的一些资源，这样的话其实就增加了 struct tpt_bg 设计的复杂度，
但感觉这点开销还是能够接受的。或者我们直接将块组单独放到一个区域当中，不要跟超级块混在一起，相当于是解耦了


////////////// 20220129 //////////////
  外面的打印机改了一个名字，叫 jet_pro，打印的时候要注意，最好不要选择霍世莹办公室里边的那个

  ext2 EXT2_LOCK 出现的地方主要是 alloc 功能，从代码来看是对整个文件系统进行加锁，而不是只对某个块组进行加锁。为什么？因为我们分配
数据块的时候其实是不知道会在那个块组中分配到，有可能当前块组中数据块已经被占满了，或者根据分配算法需要在别的块组中分配数据块。这个时候
我们就需要对整个文件系统进行加锁，在所有的块组中查找合适的可用数据块。所以，到底采用哪种方式需要进一步讨论


////////////// 20220130 //////////////
  FreeBSD 设计手册中对于文件系统元数据的缓存有着一段话的描述： “对于和磁盘块有关的数据块来说，虚拟内存系统没有办法描述它们。系统保留了
一块缓冲区缓存这部分磁盘块。它们保存着文件系统的元数据。包括超级块位映射图和 inode 节点”。也就是说，对于文件系统的元数据来说，操作系统
为其分配单独分配了一块缓存区域进行管理。
  我个人理解，这块缓存区域跟文件应该是分离的。对于文件数据的缓存管理，应该在 vnode 或者进程文件描述符层级，数据同步、多个文件对于同一个
文件的读写等操作都是包含的。但是磁盘相关的元数据并不属于任何一个文件，它是整个文件系统共同拥有的全局性数据。所以也就不可能通过单个文件层级
进行数据同步管理，可能就需要借助缓存机制来进行

  FreeBSD 文件系统缓存区管理章节提到，“为了保持文件系统数据的一致性，内核必须保证磁盘上的一个数据块最多只能映射到一个缓存去当中”。从这句话
中我们可以看到，内核对于磁盘块的数据同步管理很可能就是在 buffer 层级来实现的。文件的话应该就是通过文件锁来进行数据同步，因为每个文件对应的
磁盘数据块都是独立的，不可能存在一个数据块被两个文件同时占有的情况。所以，系统只需要对上层的文件锁进行控制，就能间接控制文件对应的缓存区的数据
访问。
  所以磁盘元数据的同步管理是不能通过文件锁来进行的，那貌似也就只能利用 buffer 的特性来管理了。