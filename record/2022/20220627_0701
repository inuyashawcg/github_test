////////////// 20220627 //////////////
  # mount -t tptfs tptfs /tpt
  # cd tpt
  # mkdir a
  random: unblocking device.
  # mkdir b
  # mkdir a/b
  # mv b a
  # ls -al
  total 0
  drwxr-xr-x  2 root  wheel  1024 Jun 27 03:17 .
  drwxr-xr-x  2 root  wheel  1024 Jun 27 03:17 ..
  drwxr-xr-x  4 root  wheel   768 Jun 27 03:17 a
  # ls -al a
  total 0
  drwxr-xr-x  4 root  wheel  768 Jun 27 03:17 .
  drwxr-xr-x  4 root  wheel  768 Jun 27 03:17 ..
  drwxr-xr-x  2 root  wheel  512 Jun 27 03:17 b
  #

  目标文件存在时的重命名基本功能正常，但仍然是会随机出现一些莫名的阻塞，这个原因要是要调查一下。复现条件一般是重命名一个目录项，
然后进入目标目录，退出后执行创建文件命令，此时发生阻塞的概率高一些。

  硬链接目前的设计方式存在一些问题，当我们执行删除操作的时候，不能确定这个文件到底是源文件还是目标文件。我们删除硬链接文件的时候
就只是减少文件的引用计数，但是删除源文件的时候却是要将这些文件管理的数据一并删除
  inode 中包括一个字段，存放它所指向的文件类的指针，初始指向源文件。假如给源文件创建了多个硬链接文件，然后删除了源文件，那 inode
中的指针应该指向哪个文件类实体，其实是不能确定的。或者我们可以要求用户只要存在硬链接文件，就不能删除源文件。但是如果一个文件关联的
硬链接文件非常多，那么我们就很难知道到底哪些是硬链接文件，哪个是源文件。并且对于用户来说，使用起来也是非常痛苦的

  造成这种情况的根本原因还是当前文件系统仍然保留了原有文件系统的基本设计，只是在原有基础上增加了文件类模块。目前比较理想的情况应该是
vnode 直接对应文件类，而不再经过 inode。所以，文件系统开发可能要经过三个阶段：
  1、增加文件类，用于文件注册和替换 tptfs/vfs 原有的部分功能
  2、vnode / inode / TpfFile 三者之间的关系。目前还是 vnode->inode->TptFile，应该要改成 vnode -> TptFile
  3、利用 TptFile 完整替换 vnode (这个应该是在大后期)

  # mount -t tptfs tptfs /tpt
  # cd tpt
  # mkdir -p a/b/c/d/e
  random: unblocking device.
  # cd a/b/c/d/e
  # cd ../../.
  # pwd
  /tpt/a/b/c
  # cd ././../.
  /tpt/a/b
  # pwd
  /tpt/a/b
  # cd ../b/./c/.
  # pwd
  /tpt/a/b/c
  # cd ..///.///..
  # pwd
  /tpt/a
  # cd 
  # pwd
  /


////////////// 20220628 //////////////
  vfs 提供给文件系统的接口传入的参数都是 vnode，没有具体的 inode，所以我们只需要将 vnode->v_data 的指针初设置成文件类
的指针，就可以完成 inode 和文件类之间的转换。inode 其实可以融入到文件类当中，但是感觉单独作为一个结构体也是可以的，毕竟文件
属性、数据存放等目前还都是基于 inode 来做，先不要大改吧，循序渐进

  在执行 cp 命令的时候发现了一个问题，我们只有在进入 tptfs 之后执行，才能成功。如果我们在 ufs 下拷贝文件到 tptfs 就会出现
panic。原因是我们对 namei() 中的一些辅助函数的处理只针对了 tptfs，没有兼容 ufs。感觉应该是要修改 vfs cached lookup 逻辑

  tptfs 中是否仍然需要保留 namecache？

  FreeBSD-13.0 vfs 中 cache_alloc() 貌似是有 bug 的，所以后续要根据最新的提交代码对其进行修复


////////////// 20220629 //////////////
  cache_fplookup() 实现逻辑:
    - 初始化 struct cache_fpl，包括基本的路径和属性信息
    - 调用 cache_can_fplookup() 函数，判断是否支持 fpl，包括但不限于 mount、线程
    - 调用 cache_fpl_checkpoint_outer() 保存此时的状态信息，用于在 fpl 查抄失败时的基本数据恢复
    - cache_fpl_smr_enter_initial() 大致可以认为是申请一个 smr 管理对象
    - 获取当前目录 vnode
    - 调用 cache_fplookup_impl()
    - 获取目标文件和其父目录的 vnode
    - 根据查找属性判断是否清除 struct componentname


  cache_fplookup_impl() 函数是 fpl 机制的实现函数，基本逻辑是：
    - 调用 cache_fpl_checkpoint()，保存节点的属性信息，作用与上文类似，用于查找异常情况下的数据恢复
    - 调用 cache_fplookup_mp_supported()，判断文件系统是否支持 fpl
    - for(;;) {
          - 调用 cache_fplookup_parse()，解析 cache_fpl 中保存的数据
          - cache_fplookup_next()，感觉就是利用哈希表中存放的 struct namecache 逐级向下查找
          - cache_fpl_terminated()，判断是否执行完毕。如果是，则退出循环
          - cache_fplookup_symlink()，处理当目标文件是链接文件的情况
          - directory vnode = target file vnode
          - cache_fpl_checkpoint()
      }


    # mkdir -p a/b/c/d/e
    # rm -rf a
    # ls -al
    total 0
    drwxr-xr-x  3 root  wheel  768 Jun 29 05:33 .
    drwxr-xr-x  3 root  wheel  768 Jun 29 05:33 ..
    drwxr-xr-x  2 root  wheel  768 Jun 29 05:33 a
    # cd a
    # ls
    b
    # cd b/c/d/e/
    # ls -al
    total 0
    drwxr-xr-x  1 root  wheel  512 Jun 29 05:33 .
    drwxr-xr-x  1 root  wheel  512 Jun 29 05:33 ..

    无法删除 mkdir -p 命令创建的目录


  在 tmpfs 文件系统下的执行情况：
    # mkdir -p a/b/c
    # touch a/b/c/d
    # rm -rf a
  
    Thread 1 hit Breakpoint 5, tmpfs_remove (v=0x1def78878)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tmpfs/tmpfs_vnops.c:713
    713		struct vnode *dvp = v->a_dvp;
    (gdb) bt
    #0  tmpfs_remove (v=0x1def78878)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tmpfs/tmpfs_vnops.c:713
    #1  0x000000010c2542ac in VOP_REMOVE_APV (vop=0x11002c3d8 <tmpfs_vnodeop_entries>, a=0x1def78878)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vnode_if.cpp:1541
    #2  0x000000010c2416a8 in VOP_REMOVE (dvp=0xffffffd00974c7a0, vp=0xffffffd00978e1e8, cnp=0x1def78a70)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vnode_if.h:802
    #3  0x000000010c2413a6 in kern_funlinkat (td=0x1bcb66680, dfd=-100, path=0x40226778 "d", fd=-200, 
        pathseg=UIO_USERSPACE, flag=0, oldinum=0)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_syscalls.cpp:1925
    #4  0x000000010c241060 in sys_unlink (td=0x1bcb66680, uap=0x1bcb66a68)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_syscalls.cpp:1806
    #5  0x000000010c0191b6 in syscallenter (td=0x1bcb66680)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/../../kernel/subr_syscall:189
    #6  0x000000010c018ab4 in ecall_handler ()
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/trap.cpp:168
    #7  0x000000010c004fc0 in system_call ()
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/swtch.S:601

    Thread 1 hit Breakpoint 4, tmpfs_rmdir (v=0x1def78938)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tmpfs/tmpfs_vnops.c:1272
    1272		struct vnode *dvp = v->a_dvp;
    (gdb) p v
    $3 = (struct vop_rmdir_args *) 0x1def78938
    (gdb) p *v
    $4 = {
      a_gen = {
        a_desc = 0x110022cb0 <vop_rmdir_desc>
      },
      a_dvp = 0xffffffd00974c988,
      a_vp = 0xffffffd00974c7a0,
      a_cnp = 0x1def78a78
    }
    (gdb) p *v->a_cnp
    $5 = {
      cn_origflags = 335618808,
      cn_flags = 285475852,
      cn_thread = 0x1bcb66680,
      cn_cred = 0xffffffd0097c0a00,
      cn_nameiop = DELETE,
      cn_lkflags = 524288,
      cn_pnbuf = 0xffffffd00b698c00 "c",
      cn_nameptr = 0xffffffd00b698c00 "c",
      cn_namelen = 1
    }
    (gdb) c
    Continuing.

    Thread 1 hit Breakpoint 4, tmpfs_rmdir (v=0x1def78938)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tmpfs/tmpfs_vnops.c:1272
    1272		struct vnode *dvp = v->a_dvp;
    (gdb) c
    Continuing.

    Thread 1 hit Breakpoint 4, tmpfs_rmdir (v=0x1def78938)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tmpfs/tmpfs_vnops.c:1272
    1272		struct vnode *dvp = v->a_dvp;
    (gdb) p *v->a_cnp
    $6 = {
      cn_origflags = 0,
      cn_flags = 285475852,
      cn_thread = 0x1bcb66680,
      cn_cred = 0xffffffd0097c0a00,
      cn_nameiop = DELETE,
      cn_lkflags = 524288,
      cn_pnbuf = 0xffffffd00b698c00 "a",
      cn_nameptr = 0xffffffd00b698c00 "a",
      cn_namelen = 1
    }

  可以看到，执行 rm -rf a 的命令其实是递归删除。首先是将最后一个子目录中的文件删除 (remove)，然后依次删除 c/b/a (rmdir)。
但是在 tptfs 中却没有出现这样的调用情况，需要排查具体原因。

  tptfs 执行上述命令的时候，没有触发 sys_rmdir。ufs 实际调试跟 tmpfs 机制比较类似，都是递归查找目录下的所有文件。完成之后
开始由下到上依次删除普通文件和目录。所以对文件的操作类型大致是先 LOOKUP，然后再回头 DELETE。tptfs 中就比较奇怪，只有前面的
LOOKUP，没有后面的 DELETE


////////////// 20220630 //////////////
在 rm.c 中给 rm_tree() 函数打断点，查看 fts 中保存的数据：

  Thread 2 hit Breakpoint 5, rm_tree (argv=0xffffeca0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/bin/rm/rm.c:190
  190		needstat = !uid || (!fflag && !iflag && stdin_ok);
  (gdb) bt
  #0  rm_tree (argv=0xffffeca0) at /home/mercury/Documents/code/qihai/rebuild_last/qihai/bin/rm/rm.c:190
  #1  0x000000010ce5f80c in rm_main (argc=1, argv=0xffffeca0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/bin/rm/rm.c:169
  #2  0x000000010cb48622 in main (argc=3, argv=0xffffec90, env=0xffffecb0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/csu/riscv/crt1_c.c:90
  (gdb) n
  198		flags = FTS_PHYSICAL;
  (gdb) 
  199		if (!needstat)
  (gdb) 
  201		if (Wflag)
  (gdb) 
  203		if (xflag)
  (gdb) 
  205		if (!(fts = fts_open(argv, flags, NULL))) {
  (gdb) 
  210		while (errno = 0, (p = fts_read(fts)) != NULL) {
  (gdb) 
  211			switch (p->fts_info) {
  (gdb) 
  236				if (!fflag && !check(p->fts_path, p->fts_accpath,
  (gdb) p fts
  $4 = (FTS *) 0x4021c000
  (gdb) p* fts
  $5 = {
    fts_cur = 0x40226180,
    fts_child = 0x0,
    fts_array = 0x0,
    fts_dev = 18446744071679573761,
    fts_path = 0x40216500 "a",
    fts_rfd = 3,
    fts_pathlen = 1280,
    fts_nitems = 0,
    fts_compar = 0x0,
    fts_options = 16,
    fts_clientptr = 0x0
  }
fts_path 保存了我们所要删除的文件名

  211			switch (p->fts_info) {
  (gdb) 
  236				if (!fflag && !check(p->fts_path, p->fts_accpath,
  (gdb) 
  241				else if (!uid &&
  (gdb) 
  242					 (p->fts_statp->st_flags & (UF_APPEND|UF_IMMUTABLE)) &&
  (gdb) 
  247				continue;
  (gdb) 

  211			switch (p->fts_info) {
  (gdb) 
  250				if (p->fts_number == SKIPPED)
  (gdb) 
  252				break;
  (gdb) 
  259			rval = 0;
  (gdb) 
  260			if (!uid &&
  (gdb) 
  261			    (p->fts_statp->st_flags & (UF_APPEND|UF_IMMUTABLE)) &&
  (gdb) 
  265			if (rval == 0) {
  (gdb) 
  271				switch (p->fts_info) {
  (gdb) 
  274					rval = rmdir(p->fts_accpath);
  ...
这里开始会调用到底层 rmdir() 函数

  211			switch (p->fts_info) {
  (gdb) 
  236				if (!fflag && !check(p->fts_path, p->fts_accpath,
  (gdb) 
  241				else if (!uid &&
  (gdb) 
  242					 (p->fts_statp->st_flags & (UF_APPEND|UF_IMMUTABLE)) &&
  (gdb) 
  247				continue;
  (gdb) 
  333		if (!fflag && errno)
  (gdb) 
  335		fts_close(fts);
  (gdb) 
  336	}
  ...
tptfs 好像是有没有回跳到 p211 调用 rmdir，而是直接到了处理错误码的代码分支


  847		if (descend && (type == BCHILD || !nitems) &&
  (gdb) 
  848		    (cur->fts_level == FTS_ROOTLEVEL ?
  (gdb) 
  847		if (descend && (type == BCHILD || !nitems) &&
  (gdb) 
  850		    fts_safe_changedir(sp, cur->fts_parent, -1, ".."))) {
  (gdb) 
  847		if (descend && (type == BCHILD || !nitems) &&
  (gdb) 
  851			fts_lfree(head);
  (gdb) 
  852			cur->fts_info = FTS_ERR;
  (gdb) 
  853			SET(FTS_STOP);
  (gdb)


////////////// 20220701 //////////////
fts_load() 函数执行之后，就把 ftsent 中的数据加载到了 fts 结构当中

  (gdb) p sp
  $16 = (FTS *) 0x4021c000
  (gdb) p* sp
  $17 = {
    fts_cur = 0x40226300,
    fts_child = 0x0,
    fts_array = 0x0,
    fts_dev = 0,
    fts_path = 0x40216500 "",
    fts_rfd = 3,
    fts_pathlen = 1280,
    fts_nitems = 0,
    fts_compar = 0x0,
    fts_options = 16,
    fts_clientptr = 0x0
  }
  (gdb) n
  410				fts_load(sp, p);
  (gdb) 
  411				return (sp->fts_cur = p);
  (gdb) p* sp
  $18 = {
    fts_cur = 0x40226300,
    fts_child = 0x0,
    fts_array = 0x0,
    fts_dev = 18446744071679573761,
    fts_path = 0x40216500 "a",
    fts_rfd = 3,
    fts_pathlen = 1280,
    fts_nitems = 0,
    fts_compar = 0x0,
    fts_options = 16,
    fts_clientptr = 0x0
  }

  tmpfs 中执行 rm -rf a(/b/c) 命令 rm_tree() 函数的行为：
    - while(... fts_read() ...) 的第一次循环貌似只是对 fts 进行初始化，设置一下 root 地址和当前地址等字段
    - 第二次循环执行 fts_read() 函数时 fts 的状态
      (gdb) p *sp
      $24 = {
        fts_cur = 0x40226180,
        fts_child = 0x0,
        fts_array = 0x0,
        fts_dev = 18446744071679573761,
        fts_path = 0x40216500 "a",
        fts_rfd = 3,
        fts_pathlen = 1280,
        fts_nitems = 0,
        fts_compar = 0x0,
        fts_options = 16,
        fts_clientptr = 0x0
      }
      (gdb) p *sp->fts_cur
      $25 = {
        fts_cycle = 0x0,
        fts_parent = 0x40226000,
        fts_link = 0x0,
        fts_number = 0,
        fts_pointer = 0x0,
        fts_accpath = 0x40216500 "a",
        fts_path = 0x40216500 "a",
        fts_errno = 0,
        fts_symfd = 0,
        fts_pathlen = 1,
        fts_namelen = 1,
        fts_ino = 3,
        fts_dev = 18446744071679573761,
        fts_nlink = 3,
        fts_level = 0,
        fts_info = 1,
        fts_flags = 0,
        fts_instr = 3,
        fts_statp = 0x40226218,
        fts_name = 0x402262f8 "a",
        fts_fts = 0x4021c000
      }
    可以看到，第一次循环是把 fts_cur 对象中的数据加载到 fts 结构的对应字段当中。再往下走，程序会判断 fts
  的子链表是否为空，如果是，则调用 fts_build() 函数构造一个子链表：

      (gdb)  p *sp
      $28 = {
        fts_cur = 0x40226180,
        fts_child = 0x0,
        fts_array = 0x0,
        fts_dev = 18446744071679573761,
        fts_path = 0x40216500 "a",
        fts_rfd = 3,
        fts_pathlen = 1280,
        fts_nitems = 0,
        fts_compar = 0x0,
        fts_options = 16,
        fts_clientptr = 0x0
      }
      (gdb) n
      378			if (sp->fts_child != NULL) {
      (gdb) n

      Thread 7 hit Breakpoint 3, fts_build (sp=0x4021c000, type=3)
          at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/gen/fts.c:637
      637		cur = sp->fts_cur;
      (gdb) 


  从代码分析中可以看出，p 表示的其实是目录b，但是下面读取的sb中存储的信息却是目录c的，所以两者的 inode number
是不一样的，导致文件对应关系错了，最终导致文件没有被正确删除

    (gdb) p *p
    $26 = {
      fts_cycle = 0x0,
      fts_parent = 0x40226180,
      fts_link = 0x0,
      fts_number = 0,
      fts_pointer = 0x0,
      fts_accpath = 0x40226478 "b",
      fts_path = 0x40216500 "a/b/c",
      fts_errno = 0,
      fts_symfd = 0,
      fts_pathlen = 3,
      fts_namelen = 1,
      fts_ino = 4,
      fts_dev = 18446744071763459841,
      fts_nlink = 3,
      fts_level = 1,
      fts_info = 1,
      fts_flags = 0,
      fts_instr = 3,
      fts_statp = 0x40226398,
      fts_name = 0x40226478 "b",
      fts_fts = 0x4021c000
    }

    (gdb) n
    1137		if (p->fts_dev != sb.st_dev || p->fts_ino != sb.st_ino) {
    (gdb) p sb
    $27 = {
      st_dev = 18446744071763459841,
      st_ino = 5,
      st_nlink = 2,
      st_mode = 16877,
      st_padding0 = 0,
      st_uid = 0,
      st_gid = 0,
      st_padding1 = 0,
      st_rdev = 18446744073709551615,
      st_atim = {
        tv_sec = 1656661246,
        tv_nsec = 0
      },
      st_mtim = {
        tv_sec = 1656661246,
        tv_nsec = 0
      },
      st_ctim = {
        tv_sec = 1656661246,
        tv_nsec = 0
      },
      st_birthtim = {
        tv_sec = 1656661246,
        tv_nsec = 0
      },
      st_size = 512,
      st_blocks = 0,
      st_blksize = 4096,
      st_flags = 0,
      st_gen = 798667720,
      st_spare = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
    }


fts_safe_changedir() 传入的文件名是 ".."，感觉应该是要定位到父目录。但是从 stat() 反馈回来的数据看，
它得到的貌似还是当前目录文件的相关信息(inode number)。

    Thread 3 hit Breakpoint 2, fts_safe_changedir (sp=0x4021c000, p=0x40226300, fd=-1, 
        path=0x90031453 <user_lib_data_start+201811> "..")
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/gen/fts.c:1127
    1127		newfd = fd;
    (gdb) bt
    #0  fts_safe_changedir (sp=0x4021c000, p=0x40226300, fd=-1, 
        path=0x90031453 <user_lib_data_start+201811> "..")
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/gen/fts.c:1127
    #1  0x000000010ca54620 in fts_build (sp=0x4021c000, type=3)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/gen/fts.c:850
    #2  0x000000010ca538a4 in fts_read (sp=0x4021c000)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/gen/fts.c:387
    #3  0x000000010ce6042a in rm_tree (argv=0xffffeca0)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/bin/rm/rm.c:210
    #4  0x000000010ce5f85a in rm_main (argc=1, argv=0xffffeca0)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/bin/rm/rm.c:169
    #5  0x000000010cb48670 in main (argc=3, argv=0xffffec90, env=0xffffecb0)
        at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/csu/riscv/crt1_c.c:90
    (gdb) p *sp
    $24 = {
      fts_cur = 0x40226480,
      fts_child = 0x0,
      fts_array = 0x0,
      fts_dev = 18446744071763459842,
      fts_path = 0x40216500 "a/b/c",
      fts_rfd = 3,
      fts_pathlen = 1280,
      fts_nitems = 0,
      fts_compar = 0x0,
      fts_options = 16,
      fts_clientptr = 0x0
    }
    (gdb) p sp->fts_cur 
    $25 = (struct _ftsent *) 0x40226480
    (gdb) p *sp->fts_cur 
    $26 = {
      fts_cycle = 0x0,
      fts_parent = 0x40226300,
      fts_link = 0x0,
      fts_number = 0,
      fts_pointer = 0x0,
      fts_accpath = 0x402265f8 "c",
      fts_path = 0x40216500 "a/b/c",
      fts_errno = 0,
      fts_symfd = 0,
      fts_pathlen = 5,
      fts_namelen = 1,
      fts_ino = 5,
      fts_dev = 18446744071763459842,
      fts_nlink = 2,
      fts_level = 2,
      fts_info = 1,
      fts_flags = 0,
      fts_instr = 3,
      fts_statp = 0x40226518,
      fts_name = 0x402265f8 "c",
      fts_fts = 0x4021c000
    }
    (gdb) p *p
    $27 = {
      fts_cycle = 0x0,
      fts_parent = 0x40226180,
      fts_link = 0x0,
      fts_number = 0,
      fts_pointer = 0x0,
      fts_accpath = 0x40226478 "b",
      fts_path = 0x40216500 "a/b/c",
      fts_errno = 0,
      fts_symfd = 0,
      fts_pathlen = 3,
      fts_namelen = 1,
      fts_ino = 4,
      fts_dev = 18446744071763459842,
      fts_nlink = 3,
      fts_level = 1,
      fts_info = 1,
      fts_flags = 0,
      fts_instr = 3,
      fts_statp = 0x40226398,
      fts_name = 0x40226478 "b",
      fts_fts = 0x4021c000
    }
    (gdb) n
    1128		if (ISSET(FTS_NOCHDIR))
    (gdb) 
    1130		if (fd < 0 && (newfd = _open(path, O_RDONLY | O_DIRECTORY |
    (gdb) p path
    $28 = 0x90031453 <user_lib_data_start+201811> ".."
    (gdb) n
    1133		if (_fstat(newfd, &sb)) {
    (gdb) p newfd
    $29 = 4
    (gdb) p sb
    $30 = {
      st_dev = 1075839056,
      st_ino = 2416753064,
      st_nlink = 1075839008,
      st_mode = 16877,
      st_padding0 = 256,
      st_uid = 30,
      st_gid = 34,
      st_padding1 = 64,
      st_rdev = 1075921792,
      st_atim = {
        tv_sec = -1073741824,
        tv_nsec = 128849018880
      },
      st_mtim = {
        tv_sec = 274877906978,
        tv_nsec = 1075921792
      },
      st_ctim = {
        tv_sec = 262143,
        tv_nsec = 77309411328
      },
      st_birthtim = {
        tv_sec = 223338299404,
        tv_nsec = 4294967360
      },
      st_size = 1075921792,
      st_blocks = 72057594037927936,
      st_blksize = 1077940576,
      st_flags = 0,
      st_gen = 2416753064,
      st_spare = {1075839008, 1688850934047489, 0, 1077940576, 0, 1688850934047489, 
        1688850934047489, 4294961608, 4507074552, 1076019232}
    }
    (gdb) n
    1137		if (p->fts_dev != sb.st_dev || p->fts_ino != sb.st_ino) {
    (gdb) p sb
    $31 = {
      st_dev = 18446744071763459842,
      st_ino = 5,
      st_nlink = 2,
      st_mode = 16877,
      st_padding0 = 0,
      st_uid = 0,
      st_gid = 0,
      st_padding1 = 0,
      st_rdev = 18446744073709551615,
      st_atim = {
        tv_sec = 1656665882,
        tv_nsec = 0
      },
      st_mtim = {
        tv_sec = 1656665882,
        tv_nsec = 0
      },
      st_ctim = {
        tv_sec = 1656665882,
        tv_nsec = 0
      },
      st_birthtim = {
        tv_sec = 1656665882,
        tv_nsec = 0
      },
      st_size = 512,
      st_blocks = 0,
      st_blksize = 4096,
      st_flags = 0,
      st_gen = 2630448202,
      st_spare = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
    }
    (gdb) p p->fst_ino
    There is no member named fst_ino.
    (gdb) p *p        
    $32 = {
      fts_cycle = 0x0,
      fts_parent = 0x40226180,
      fts_link = 0x0,
      fts_number = 0,
      fts_pointer = 0x0,
      fts_accpath = 0x40226478 "b",
      fts_path = 0x40216500 "a/b/c",
      fts_errno = 0,
      fts_symfd = 0,
      fts_pathlen = 3,
      fts_namelen = 1,
      fts_ino = 4,
      fts_dev = 18446744071763459842,
      fts_nlink = 3,
      fts_level = 1,
      fts_info = 1,
      fts_flags = 0,
      fts_instr = 3,
      fts_statp = 0x40226398,
      fts_name = 0x40226478 "b",
      fts_fts = 0x4021c000
    }

  从实际调试的情况来看，感觉每次到了输入 .. 的时候就会产生问题，只有前年 preorder 的处理过程，后面
postorder 的处理过程因为报错导致无法正常执行。所以，猜测有可能是 lookup() 对于 .. 的处理存在异常