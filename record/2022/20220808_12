////////////// 20220808 //////////////
上周梅潮提出了一个新的文件系统设计思路，就是把虚拟文件也封装成一个类对象，然后挂载根文件系统的时候就是
  class VirtualFileSystem->mountRoot(class TptFileSystem)

  目前 vfs 层级是通过一个数组将所有类型的文件系统都放到一个队列中处理，貌似队列中的第一个元素就代表根文件系统类型。
感觉两者的处理方式差不多，只不过一个是通过函数参数直接传递进去，vfs 是通过拿到队列中的第一个元素。

  如果我们要独立设计一个 vfs 类对象，里面包含的应该不是所有的函数，比如 default functions 还是可以放到文件系统基类
当中。不是特定于某个文件系统的函数，比如 mountroot，就可以放到 vfs 类对象当中

  上周将 class TptFile 类当中的一些简单的成员函数显式定义成了 inline 类型，发现编译的时候会提示： undefined reference to `***``
造成此类错误的原因是 inline 定义的成员函数的实现一定要跟函数声明在同一个源文件中，也就是都要放在 .h 文件当中


  PersistentMemory::initialize (this=0x1102909c0 <persistentMemory>)
    at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/pm/PersistentMemory.cpp:111
  111	        kc_printf("namei(ndp) error.\n");
  (gdb) n
  112	    NDFREE(ndp, NDF_ONLY_PNBUF);
  (gdb) n
  113	    devvp = ndp->ni_vp;
  (gdb) 
  115	    g_topology_lock();
  (gdb) p devvp
  $1 = (vnode *) 0x0


  将 tptfs 作为根文件系统，首先要把文件系统对应的持久化内存进行初始化，时机一定要选对，不能在 vfs mountroot() 函数之后。
因为该函数会调用 tptfs_mount，如果放在后面，就会导致挂载的时候找不到对应的持久化内存区域，panic。但是如果直接放在 mountroot()
前面，也不行。因为系统加载设备可能需要花一段时间，所以为调用下面这个函数

  vfs_mountroot_wait_if_neccessary()

  从函数命名可以看出，如果必要的话就等待，猜测是等待设备加载完毕。所以，如果持久化内存初始化操作过于靠前，可能会发生找不到设备文件的
情况。综合考虑，放在 wait_if 函数之后是比较合适的


  panic: Fatal page fault at 0x10c3851f8: 0x00000000000050
  cpuid = 7
  time = 3
  KDB: stack backtrace:
  #0 0x10c179f50 at kdb_backtrace+0x7c
  #1 0x10c114f50 at vpanic+0x202
  #2 0x10c114d4a at panic+0x32
  #3 0x10c019d64 at do_trap_supervisor+0x5da
  #4 0x10c019860 at do_trap_supervisor+0xd6
  #5 0x10c004508 at cpu_exception_handler_supervisor+0x68
  #6 0x10c388f9e at _ZN13TptFileSystem17constructTreeNodeEPciS0_i+0x24e
  #7 0x10c388c50 at _ZN13TptFileSystem15parseRecordFileEv+0x1b8
  #8 0x10c38841a at _ZN13TptFileSystem14createFileTreeEv+0x190
  #9 0x10c387ecc at _ZN13TptFileSystem5mountEP5mount+0x27a
  #10 0x10c389350 at _Z11tptfs_mountP5mount+0x1c
  #11 0x10c222cea at resume_all_fs+0xd06
  #12 0x10c21dd1a at vfs_getopt_pos+0x44a
  #13 0x10c21c9f8 at vfs_donmount+0xa74
  #14 0x10c2219e6 at kernel_mount+0x6e
  #15 0x10c22767e at vfs_mountroot+0x25e4
  #16 0x10c225f6a at vfs_mountroot+0xed0
  #17 0x10c22510c at vfs_mountroot+0x72
  Uptime: 3s

  OpenSBI v0.9


////////////// 20220809 //////////////
  Trying to mount root from tptfs:/dev/vtbd1 []...
  mountroot: unable to remount devfs under /dev (error 20)
  mountroot: unable to unlink /dev/dev (error 2)
  panic: vm_fault_lookup: fault on nofault entry, addr: 0x11414a000
  cpuid = 7
  time = 2
  KDB: stack backtrace:
  #0 0x10c179f8c at kdb_backtrace+0x7c
  #1 0x10c114f8c at vpanic+0x202
  #2 0x10c114d86 at panic+0x32
  #3 0x10c291bbc at vm_fault+0xdc4
  #4 0x10c290e9c at vm_fault+0xa4
  #5 0x10c290c22 at vm_fault_trap+0x86
  #6 0x10c019c2e at do_trap_supervisor+0x4a4
  #7 0x10c019860 at do_trap_supervisor+0xd6
  #8 0x10c004508 at cpu_exception_handler_supervisor+0x68
  #9 0x10c38ccc8 at _ZN9TptLookup6lookupEP9nameidata+0x2de
  #10 0x10c389476 at _Z10tpt_lookupP9nameidata+0x24
  #11 0x10c05ed36 at _ZN9VfsLookup5nameiEP9nameidata+0x29e
  #12 0x10c061678 at namei+0x1c
  #13 0x10c249874 at vn_open_cred+0x430
  #14 0x10c249438 at vn_open+0x40
  #15 0x10c226cfa at vfs_mountroot+0x1c24
  #16 0x10c225186 at vfs_mountroot+0xb0
  #17 0x10c06d646 at mi_startup+0x2294
  Uptime: 2s


  系统打印完 start_init 后阻塞，首先判断是否进入 main 函数，可以在
    __start
    main
    exec
  等函数处打断点

  i thread 查看进程情况

  通过添加断点可以看到，open_console() 函数已经被调用，就是没有界面显示出来。这个函数回去查找 /dev/console 设备，猜测是没有
正确找到设备文件，导致 shell 界面加载异常。根本原因应该还是 devfs 挂载异常导致的


////////////// 20220810 //////////////
  (gdb) p* pwd     
  $12 = {
    pwd_refcount = 2,
    pwd_cdir = 0xffffffd00a806d58,
    pwd_rdir = 0xffffffd00a806d58,
    pwd_jdir = 0x0
  }
  (gdb) bt
  #0  VfsLookup::namei (this=0x1100db038 <vfsLookup>, ndp=0x11414e688)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookup.cpp:508
  #1  0x000000010c061640 in namei (ndp=0x11414e688)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookupWrapper.cpp:15
  #2  0x000000010c2286e2 in parse_mount_dev_present (dev=0xffffffd00a805f34 "/dev/vtbd0")
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:719
  #3  0x000000010c228366 in vfs_mountroot_wait_if_neccessary (fs=0xffffffd00a805f30 "ufs", 
      dev=0xffffffd00a805f34 "/dev/vtbd0")
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:1023
  #4  0x000000010c227552 in parse_mount (conf=0x11414e8e8)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:770
  #5  0x000000010c225f6e in vfs_mountroot_parse (sb=0xffffffd00972ef00, mpdevfs=0x1bcada040)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:847
  #6  0x000000010c225110 in vfs_mountroot ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:1066
  #7  0x000000010c06d60e in start_init (dummy=0x0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/init_main.cpp:675
  #8  0x000000010c0b1318 in fork_exit (callout=0x10c06d5e4 <start_init(void*)>, arg=0x0, frame=0x11414ec50)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/kern_fork.cpp:1045
  #9  0x000000010c004d8e in fork_trampoline ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/swtch.S:385

以 ufs 作为根文件系统挂载时 pwd 当前目录和根目录都是 devfs vnode 类型


  #0  VfsLookup::namei (this=0x1100db038 <vfsLookup>, ndp=0x11414e558)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookup.cpp:509
  #1  0x000000010c061640 in namei (ndp=0x11414e558)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookupWrapper.cpp:15
  #2  0x000000010c21dc02 in vfs_domount (td=0x115ab0100, fstype=0xffffffd00a805f00 "ufs", fspath=0xffffffd00a805ee0 "/", 
      fsflags=16385, optlist=0x11414e718)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mount.cpp:1344
  #3  0x000000010c21c9fc in vfs_donmount (td=0x115ab0100, fsflags=16385, fsoptions=0x11414e760)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mount.cpp:780
  #4  0x000000010c2219ea in kernel_mount (ma=0xffffffd00b62bd40, flags=16384)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mount.cpp:2438
  #5  0x000000010c227682 in parse_mount (conf=0x11414e8e8)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:787
  #6  0x000000010c225f6e in vfs_mountroot_parse (sb=0xffffffd00972ef00, mpdevfs=0x1bcada040)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:847
  #7  0x000000010c225110 in vfs_mountroot ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:1066
  #8  0x000000010c06d60e in start_init (dummy=0x0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/init_main.cpp:675
  #9  0x000000010c0b1318 in fork_exit (callout=0x10c06d5e4 <start_init(void*)>, arg=0x0, frame=0x11414ec50)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/kern_fork.cpp:1045
  #10 0x000000010c004d8e in fork_trampoline ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/swtch.S:385
  Backtrace stopped: frame did not save the PC
  (gdb) p pwd
  $27 = (pwd *) 0xffffffd00a803fa0
  (gdb) p* pwd
  $28 = {
    pwd_refcount = 2,
    pwd_cdir = 0xffffffd00a806d58,
    pwd_rdir = 0xffffffd00a806d58,
    pwd_jdir = 0x0
  }

真正挂载 ufs 作为根文件系统的时候，pwd 跟之前还是一样的


  如果不对 vfs lookup 进行修改，仍然采用逐级查找的方式，应该是可以比较好解决跨文件系统访问的问题。但是现在要把 devfs 也加进来，
或者以后我们要挂载别的文件系统，那么就必须要支持逐级查找的方式。

  原有以逐级查找方式的文件系统作为根文件系统，其实不用考虑设备挂载问题的，因为它们都是遵循同一种查找机制，vfs 已经支持跨文件系统的
文件访问。但是 tptfs 采用的是直接查找的方法，在不作为根文件系统的情况下是不会存在问题的。但是如果作为根文件系统，那么将会导致兼容性
问题。

vfs lookup() 与 tptfs lookup() 的关系：
  1、如果是以逐级查找方式的文件系统作为根文件系统，则 vfs_lookup() 包含 tptfs_lookup()
  2、如果是以直接查找方式的文件系统作为根文件系统，则 tptfs_lookup() 包含 vfs_lookup()
所以，目前需要做的工作是将在 vfs_lookup() 函数融入到 tptfs_lookup() 函数中。不要再想着退回到之前的逐级查找方式了！！！


为什么 tptfs lookup() 可以在跟现有机制无缝衔接？
  因为现有的机制会去处理跨文件系统访问的情况，这样当我们访问到一个挂载点的时候，vfs 会帮助我们将一些属性环境构建好，就比如说 mount
结构和 vnode 对象的替换，即将原有文件系统文件节点对应的 vnode 转换成新挂载的文件系统的 root vnode。这样我们下次再访问的时候就会
调用新文件系统注册的函数。
  所以，tptfs 需要做的就是把上述过程反过来，给那些逐级查找的文件系统挂载点构建好初始环境，然后进行跳转


通过实际调试和源码阅读，可以发现操作系统是如何处理挂载的：
  1、首先按照原文件系统的方式找到某个特定的文件节点
  2、判断该文件节点是不是挂载点
  3、如果是，进一步判断是否查找属性，即是否支持跨文件系统访问
  4、如果是，vfs 会提供相应的处理逻辑，将一些基本的数据结构 (mount / vnode) 进行替换，然后在新的文件系统查找后续文件节点
  5、如果不是，则仍然在原有文件系统中查找文件节点


ufs 查找 /dev 时的一些初始化环境，tptfs 可以根据这个进行设置
  (gdb) p* (struct inode*)ndp->ni_startdir->v_data
  $10 = {
    i_nextsnap = {
      tqe_next = 0x0,
      tqe_prev = 0x0
    },
    i_vnode = 0xffffffd00a8043d0,
    i_ump = 0xffffffd0096fbc00,
    i_dquot = {0x0, 0x0},
    i_un = {
      dirhash = 0x0,
      snapblklist = 0x0
    },
    dinode_u = {
      din1 = 0xffffffd00b67ae00,
      din2 = 0xffffffd00b67ae00
    },
    i_number = 2,
    i_flag = 1024,
    i_effnlink = 22,
    i_count = 0,
    i_endoff = 0,
    i_diroff = 0,
    i_offset = 0,
    i_nextclustercg = -1,
    i_ea_area = 0x0,
    i_ea_len = 0,
    i_ea_error = 0,
    i_ea_refs = 0,
    i_size = 1024,
    i_gen = 1188178455,
    i_flags = 0,
    i_uid = 0,
    i_gid = 0,
    i_mode = 16877,
    i_nlink = 22
  }

  (gdb) p* (struct inode*)ndp->ni_rootdir->v_data 
  $11 = {
    i_nextsnap = {
      tqe_next = 0x0,
      tqe_prev = 0x0
    },
    i_vnode = 0xffffffd00a8043d0,
    i_ump = 0xffffffd0096fbc00,
    i_dquot = {0x0, 0x0},
    i_un = {
      dirhash = 0x0,
      snapblklist = 0x0
    },
    dinode_u = {
      din1 = 0xffffffd00b67ae00,
      din2 = 0xffffffd00b67ae00
    },
    i_number = 2,
    i_flag = 1024,
    i_effnlink = 22,
    i_count = 0,
    i_endoff = 0,
    i_diroff = 0,
    i_offset = 0,
    i_nextclustercg = -1,
    i_ea_area = 0x0,
    i_ea_len = 0,
    i_ea_error = 0,
    i_ea_refs = 0,
    i_size = 1024,
    i_gen = 1188178455,
    i_flags = 0,
    i_uid = 0,
    i_gid = 0,
    i_mode = 16877,
    i_nlink = 22
  }

  (gdb) bt
  #0  VfsLookup::namei (this=0x1100db038 <vfsLookup>, ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookup.cpp:503
  #1  0x000000010c06160a in namei (ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookupWrapper.cpp:15
  #2  0x000000010c241248 in kern_funlinkat (td=0x115ab0100, dfd=-100, path=0x10d1048bf "/dev/dev", fd=-200, 
      pathseg=UIO_SYSSPACE, flag=0, oldinum=0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_syscalls.cpp:1877
  #3  0x000000010c226b94 in vfs_mountroot_shuffle (td=0x115ab0100, mpdevfs=0x1bcad7040)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:415
  #4  0x000000010c2250fc in vfs_mountroot ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:1070
  #5  0x000000010c06d5d8 in start_init (dummy=0x0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/init_main.cpp:675
  #6  0x000000010c0b12e2 in fork_exit (callout=0x10c06d5ae <start_init(void*)>, arg=0x0, frame=0x11414ec50)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/kern_fork.cpp:1045
  #7  0x000000010c004d8e in fork_trampoline ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/swtch.S:385
  Backtrace stopped: frame did not save the PC

  gdb) p *ndp            
  $23 = {
    ni_dirp = 0x10d1048bf "/dev/dev",
    ni_segflg = UIO_SYSSPACE,
    ni_rightsneeded = 0x110088458 <cap_unlinkat_rights>,
    ni_startdir = 0x0,
    ni_rootdir = 0xffffffd00a8043d0,
    ni_topdir = 0x0,
    ni_dirfd = -100,
    ni_lcf = 0,
    ni_filecaps = {
      fc_rights = {
        cr_rights = {0, 0}
      },
      fc_ioctls = 0x0,
      fc_nioctls = -1,
      fc_fcntls = 0
    },
    ni_vp = 0x0,
    ni_dvp = 0x115ab0100,
    ni_resflags = 1,
    ni_debugflags = 65488,
    ni_loopcnt = 0,
    ni_pathlen = 8,
    ni_next = 0x115ab0100 "",
    ni_cnd = {
      cn_origflags = 4631881280,
      cn_flags = 262156,
      cn_thread = 0x115ab0100,
      cn_cred = 0xffffffd000c94900,
      cn_nameiop = DELETE,
      cn_lkflags = 1,
      cn_pnbuf = 0xffffffd00a800800 "/dev/dev",
      cn_nameptr = 0xffffffd00a800801 "dev/dev",
      cn_namelen = 0
    },
    ni_cap_tracker = {
      tqh_first = 0x0,
      tqh_last = 0x11414e650
    }
  }

  (gdb) p *ndp->ni_rootdir
  $22 = {
    v_type = VDIR,
    v_irflag = 0,
    v_seqc = 2,
    v_nchash = 1972156343,
    v_op = 0x110033c60 <ffs_vnodeops2>,
    v_data = 0xffffffd00b67bf00,
    v_mount = 0x1bcad9080,
    v_nmntvnodes = {
      tqe_next = 0xffffffd009778d58,
      tqe_prev = 0x1bcad9108
    },
    {
      v_mountedhere = 0x0,
      v_unpcb = 0x0,
      v_rdev = 0x0,
      v_fifoinfo = 0x0
    },
    v_hashlist = {
      le_next = 0x0,
      le_prev = 0x18920ec10
    },
    v_cache_src = {
      lh_first = 0xffffffd009775f70
    },
    v_cache_dst = {
      tqh_first = 0x0,
      tqh_last = 0xffffffd00a804428
    },
    v_cache_dd = 0x0,
    v_lock = {
      lock_object = {
        lo_name = 0x10d0d944a "ufs",
        lo_flags = 117112832,
        lo_data = 0,
        lo_witness = 0x0
      },
      lk_lock = 1,
      lk_exslpfail = 0,
      lk_timo = 6,
      lk_pri = 96
    },
    v_interlock = {
      lock_object = {
        lo_name = 0x10d12a62e "vnode interlock",

通过上述调试，感觉可以使用
  nameidata->ni_startdir = tptfs /dev vnode
  nameidata->ni_rootdir = tptfs / vnode

与此同时，还应该把 componentname->cn_nameptr, cn_namelen 等参数进行更新，然后直接调用 vfs lookup() 函数应该就可以了


////////////// 20220811 //////////////
  #0  devfs_lookup (ap=0x11414dd10)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/devfs/devfs_vnops.c:1187
  #1  0x000000010c38fe86 in VOP_LOOKUP (dvp=0xffffffd0096dad58, vpp=0x11414e5e8, cnp=0x11414e610)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vnode_if.h:65
  #2  0x000000010c38ddc2 in TptLookup::lookupLBL (this=0xffffffd00b61cc60, ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tptfs/TptLookup.cpp:974
  #3  0x000000010c38ca44 in TptLookup::lookupFile (this=0xffffffd00b61cc60, NameiData=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tptfs/TptLookup.cpp:35
  #4  0x000000010c389440 in tpt_lookup (ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/fs/tptfs/TptFileSystem.cpp:728
  #5  0x000000010c05eccc in VfsLookup::namei (this=0x1100db038 <vfsLookup>, ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookup.cpp:505
  #6  0x000000010c06160a in namei (ndp=0x11414e590)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/VfsLookupWrapper.cpp:15
  #7  0x000000010c241278 in kern_funlinkat (td=0x115ab0100, dfd=-100, path=0x10d105f80 "/dev/dev", 
      fd=-200, pathseg=UIO_SYSSPACE, flag=0, oldinum=0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_syscalls.cpp:1877
  #8  0x000000010c226bc4 in vfs_mountroot_shuffle (td=0x115ab0100, mpdevfs=0x1bcada040)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:415
  #9  0x000000010c22512c in vfs_mountroot ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/vfs_mountroot.cpp:1070
  #10 0x000000010c06d608 in start_init (dummy=0x0)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/init_main.cpp:675
  #11 0x000000010c0b1312 in fork_exit (callout=0x10c06d5de <start_init(void*)>, arg=0x0, 
      frame=0x11414ec50)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/kernel/kern_fork.cpp:1045
  #12 0x000000010c004d8e in fork_trampoline ()
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/sys/riscv/riscv/swtch.S:385
  Backtrace stopped: frame did not save the PC

  貌似已经可以跳转到 devfs 注册函数表当中了


  Trying to mount root from tptfs:/dev/vtbd1 []...
  goldfish_rtc0: providing initial system time
  start init
  2022-08-11T05:38:42.682691+00:00 - init 1 - - Can't stat /dev: No such file or directory
  2022-08-11T05:38:42.840674+00:00 - init 26 - - login_getclass: unknown class 'daemon'
  2022-08-11T05:38:42.859916+00:00 - init 26 - - can't access /etc/rc: No such file or directory
  sh: The terminal database could not be opened.
  sh: using dumb terminal settings.
  t[0] == 0x000000000000000d
  t[1] == 0x0000000040000980
  t[2] == 0x0000000040200020
  t[3] == 0x00000000ffff69f0
  t[4] == 0x000000004000c900
  t[5] == 0x0000000000000000
  t[6] == 0x000000010c006e42
  s[0] == 0x00000001def66090
  s[1] == 0x000000000000000e
  s[2] == 0x000000000000001e
  s[3] == 0x000000000000001f
  s[4] == 0x000000000000001c
  s[5] == 0x0000000000000000
  s[6] == 0x0000000000000000
  s[7] == 0x0000000000000000
  s[8] == 0x0000000000000000
  s[9] == 0x0000000000000000
  s[10] == 0x0000000000000000
  s[11] == 0x0000000000000000
  a[0] == 0x206c616e696d7265
  a[1] == 0x0000000000000000
  a[2] == 0x000000000000000f
  a[3] == 0x00000001def66008
  a[4] == 0x0000000000000001
  a[5] == 0x0000000000000021
  a[6] == 0x000000010c004310
  a[7] == 0x00000001def66d68
  ra == 0x000000010c38a198
  sp == 0x00000001def66070
  gp == 0x00000001def65f90
  tp == 0xffffffd0096e9c78
  sepc == 0x000000010c384e6a
  sstatus == 0x8000000000006120
  panic: Fatal page fault at 0x10c384e6a: 0x206c616e696d72a5
  cpuid = 6
  time = 1660196323
  KDB: stack backtrace:
  #0 0x10c179f4a at kdb_backtrace+0x7c
  #1 0x10c114f4a at vpanic+0x202
  #2 0x10c114d44 at panic+0x32
  #3 0x10c019d64 at do_trap_supervisor+0x5da
  #4 0x10c019860 at do_trap_supervisor+0xd6
  #5 0x10c004508 at cpu_exception_handler_supervisor+0x68
  #6 0x10c38a194 at _ZN12TptHashTable10getElementEmtt+0x6e
  #7 0x10c38ef62 at _ZN9TptLookup15handleNameiOptsEPciP9nameidata+0x132
  #8 0x10c38cd00 at _ZN9TptLookup10lookupFileEP9nameidata+0x350
  #9 0x10c38943c at _Z10tpt_lookupP9nameidata+0x24
  #10 0x10c05ecc8 at _ZN9VfsLookup5nameiEP9nameidata+0x266
  #11 0x10c061606 at namei+0x1c
  #12 0x10c24983e at vn_open_cred+0x430
  #13 0x10c249402 at vn_open+0x40
  #14 0x10c23f46a at kern_openat+0x28a
  #15 0x10c23f1d4 at sys_open+0x2a
  #16 0x10c0191b2 at ecall_handler+0x71e
  #17 0x10c018ab0 at ecall_handler+0x1c
  Uptime: 3s


  kern_openat: /etc/login.conf.db
  kern_openat: /dev/null
  kern_openat: /etc/nsswitch.conf
  kern_openat: /etc/spwd.db
  kern_openat: /etc/group
  kern_openat: /etc/csh.cshrc
  kern_openat: /sbin
  kern_openat: /bin
  kern_openat: /usr/sbin
  kern_openat: /usr/bin
  kern_openat: /.tcshrc
  kern_openat: /.cshrc
  kern_openat: /sbin
  kern_openat: /bin
  kern_openat: /usr/sbin
  kern_openat: /usr/bin
  kern_openat: /usr/local/sbin
  kern_openat: /usr/local/bin
  kern_openat: //bin
  kern_openat: /.history
  sh: The terminal database could not be opened.
  sh: using dumb terminal settings.
  You have mail.
  kern_openat: /etc/localtime
  kern_openat: /usr/share/zoneinfo/UTC
  kern_openat: /usr/share/zoneinfo/posixrules

  Thread 6 hit Breakpoint 4, stat (path=0xffffe440 "/usr/share/locale/en/LC_MESSAGES/tcsh.cat", 
    sb=0xffffe360) at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/sys/stat.c:44
  44		if (__getosreldate() >= INO64_FIRST)
  (gdb) c
  Continuing.

  Thread 6 hit Breakpoint 4, stat (path=0xffffe2c0 "/usr/share/nls/C/tcsh.cat", sb=0xffffe720)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/sys/stat.c:44
  44		if (__getosreldate() >= INO64_FIRST)
  (gdb) c
  Continuing.

  Thread 6 hit Breakpoint 4, stat (path=0xffffe2c0 "/usr/share/nls/tcsh/C", sb=0xffffe720)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/sys/stat.c:44
  44		if (__getosreldate() >= INO64_FIRST)
  (gdb) c
  Continuing.

  Thread 6 hit Breakpoint 4, stat (path=0xffffe2c0 "/usr/local/share/nls/C/tcsh.cat", sb=0xffffe720)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/sys/stat.c:44
  44		if (__getosreldate() >= INO64_FIRST)
  (gdb) c
  Continuing.

  Thread 6 hit Breakpoint 4, stat (path=0xffffe2c0 "/usr/local/share/nls/tcsh/C", sb=0xffffe720)
      at /home/mercury/Documents/code/qihai/rebuild_last/qihai/lib/libc/sys/stat.c:44
  44		if (__getosreldate() >= INO64_FIRST)

  如果是路径信息中只要一个单纯的 /dev， 和 /dev/*** 两种处理的逻辑应该是不一样的。第一种应该可以直接返回 devfs vnode，
第二种方式则是需要在逐级查找中再去找另外以文件节点，所以这里可能是一个 bug 点。第一种感觉应该是做成一个单独的循环函数，专门
处理 devfs 或者其他文件系统挂载。其他情况下，只需要设置好初始变量，然后按照原有函数的处理逻辑执行


////////////// 20220812 //////////////
terminfo 在奇海中源代码中的实际位置：
  /qihai/lib/ncurses/misc/terminfo.src

  ncurses_cfg.h 文件中，刘强定义了两个绝对本地路径信息，导致 tptfs 在启动的时候找不到相应的文件而引发 panic。因为 ufs 不会
生成文件类，但是 tptfs 却默认目标文件的目录是存在文件类的 (前序遍历生成文件树)。所以会有访问空指针的问题。解决方法就是将本地绝对
路径改成奇海系统中的绝对路径，然后在根文件系统中添加相应的路径文件

tptfs 根文件系统挂载时的打印信息：
  namei: /dev
  namei: /dev/dev
  namei: /.mount.conf
  goldfish_rtc0: providing initial system time
  start init
  namei: /etc/malloc.conf
  namei: /
  namei: /dev
  namei: /dev/reroot
  namei: /dev/reroot
  namei: /etc/login.conf
  namei: /etc/login.conf.db
  namei: /etc/login.conf
  namei: /etc/login.conf
  namei: /etc/localtime
  namei: /usr/share/zoneinfo/UTC
  namei: /usr/share/zoneinfo/posixrules
  namei: /var/run/logpriv
  namei: /var/run/log
  namei: /dev/log
  namei: /var/run/logpriv
  namei: /var/run/log
  namei: /dev/log
  namei: /dev/console
  2022-08-12T02:37:09.777204+00:00 - init 26 - - login_getclass: unknown class 'daemon'
  namei: /etc/login.conf.db
  namei: /etc/login.conf
  namei: /etc/login.conf
  namei: /etc/login.conf.db
  namei: /etc/login.conf
  namei: /etc/rc
  namei: /usr/share/nls/C/libc.cat
  namei: /usr/share/nls/libc/C
  namei: /usr/local/share/nls/C/libc.cat
  namei: /usr/local/share/nls/libc/C
  namei: /var/run/logpriv
  namei: /var/run/log
  namei: /dev/log
  namei: /var/run/logpriv
  namei: /var/run/log
  namei: /dev/log
  namei: /dev/console
  2022-08-12T02:37:09.850645+00:00 - init 26 - - can't access /etc/rc: No such file or directory
  namei: /etc/malloc.conf
  namei: /usr/share/locale/en/LC_MESSAGES/tcsh.cat
  namei: /usr/share/nls/C/tcsh.cat
  namei: /usr/share/nls/tcsh/C
  namei: /usr/local/share/nls/C/tcsh.cat
  namei: /usr/local/share/nls/tcsh/C
  namei: /dev/null
  namei: /
  namei: /
  namei: /etc/nsswitch.conf
  namei: /etc/spwd.db
  namei: /etc/pwd.db
  namei: /etc/spwd.db
  namei: /etc/pwd.db
  namei: /etc/group
  namei: /etc/csh.cshrc
  namei: /sbin
  namei: /bin
  namei: /usr/sbin
  namei: /usr/bin
  namei: /.tcshrc
  namei: /.cshrc
  namei: /.history
  namei: //.terminfo
  namei: /home/neptune/Downloads/ncurses-6.3/build/share/terminfo


ufs 作为根文件系统时的打印信息：
  namei: /dev/vtbd0
  namei: /dev/vtbd0
  namei: /dev/vtbd1
  namei: /dev/vtbd1
  namei: /
  namei: /
  namei: /dev/vtbd0
  namei: /dev/vtbd0
  WARNING: / was not properly dismounted
  namei: /dev
  namei: /dev/dev
  namei: /.mount.conf
  goldfish_rtc0: providing initial system time
  start init
  namei: /etc/malloc.conf
  namei: /
  namei: /dev
  namei: /dev/reroot
  namei: /dev/reroot
  namei: /etc/login.conf
  namei: /etc/login.conf.db
  namei: /etc/login.conf
  namei: /etc/login.conf.db
  namei: /etc/rc
  namei: /etc/malloc.conf
  namei: /usr/share/locale/en/LC_MESSAGES/tcsh.cat
  namei: /usr/share/nls/C/tcsh.cat
  namei: /usr/share/nls/tcsh/C
  namei: /usr/local/share/nls/C/tcsh.cat
  namei: /usr/local/share/nls/tcsh/C
  namei: /dev/null
  namei: /
  namei: /
  namei: /etc/nsswitch.conf
  namei: /etc/nsswitch.conf
  namei: /etc/spwd.db
  namei: /etc/spwd.db
  namei: /etc/group
  namei: /etc/csh.cshrc
  namei: /sbin
  namei: /bin
  namei: /usr/sbin
  namei: /usr/bin
  namei: /.tcshrc
  namei: /.cshrc
  namei: /sbin
  namei: /bin
  namei: /usr/sbin
  namei: /usr/bin
  namei: /usr/local/sbin
  namei: /usr/local/bin
  namei: //bin
  namei: /.history
  namei: //.terminfo
  namei: /home/neptune/Downloads/ncurses-6.3/build/share/terminfo
  namei: /home/neptune/Downloads/ncurses-6.3/build/share/terminfo
  sh: The terminal database could not be opened.
  sh: using dumb terminal settings.
  namei: /var/mail/root
  You have mail.
  namei: /etc/localtime
  namei: /usr/share/zoneinfo/UTC
  namei: /usr/share/zoneinfo/posixrules

可以看到，tptfs 挂载时貌似需要打开 /dev/console 设备，但是 ufs 是不用的，这一点比较奇怪


  namei: //.terminfo
  namei: /lib/ncurses/misc
  sh: The terminal database could not be opened.
  sh: using dumb terminal settings.
  namei: /etc/localtime
  namei: /usr/share/zoneinfo/UTC
  namei: /usr/share/zoneinfo/posixrules
  # 

  貌似进来了。。。但给人的感觉就是非常不稳定，有时候直接在 UTC 检测的时候挂掉了，有的时候又可以直接进来


  # ls
  .cshrc		csh.cshrc	group		login.conf	rc
  FilesRecord.txt	dev		localtime	login.conf.db	spwd.db
  bin		etc		login.access	nsswitch.conf
  # cd etc
  etc: No such file or directory.
  # pwd
  /
  # cd dev
  dev: No such file or directory.
  # ls -al
  ls: .cshrc: No such file or directory
  ls: FilesRecord.txt: No such file or directory
  ls: bin: No such file or directory
  ls: csh.cshrc: No such file or directory
  ls: dev: No such file or directory
  ls: etc: No such file or directory
  ls: group: No such file or directory
  ls: localtime: No such file or directory
  ls: login.access: No such file or directory
  ls: login.conf: No such file or directory
  ls: login.conf.db: No such file or directory
  ls: nsswitch.conf: No such file or directory
  ls: rc: No such file or directory
  ls: spwd.db: No such file or directory
  total 0
  drwxr-xr-x  2 0  0  7936 Jan  1  1970 .
  drwxr-xr-x  2 0  0  7936 Jan  1  1970 ..
  # ll
  /sbin/ll: Operation not permitted.
  # ls -al
  #
