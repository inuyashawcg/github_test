////////////// 20220228 //////////////
  《FreeBSD设计与实现》中有着这么一段描述：“对于和磁盘有关的数据块来说，虚拟内存系统没有办法描述它们。系统留了一小块缓冲区
来存放这些磁盘块，它们包含着文件系统的元数据，比如超级块，位映射表和 inode 节点”。从中我们可以看到，磁盘文件系统其实是将元数据
当做是一个整体来处理的，同步时使用的应该都是 buffer object 中的读写锁

  tptfs 中可以参考这种设计，因为当我们需要为新文件分配一个 inode 时，可能还没有实例化一个文件类，此时也就不存在文件锁，也就无法
利用文件锁对 inode table 或者 bitmap 进行数据同步管理，这就需要利用文件系统的读写锁来实现。所以，在 tptfs 中加锁的时候，我们
首先要判断 bread/bwrite 的操作对象是谁。如果是文件本身的数据，那么我们就用文件锁；如果处理的是文件系统的数据，即元数据，那么我们
就使用文件系统的读写锁来处理

  块组结构在 ext2fs 中是在超级块结构内，利用 ext_mount->ump_mtx 来进行同步。tptfs 中其实有两种选择，第一种是仍然采用 ext2 的
机制，利用 tptfs_mount 中的互斥锁管理；第二种方式是利用文件系统锁来管理。可能这个结构在管理程度上要严格一些，所以 ext2 直接用的
互斥锁，tptfs 暂时先沿用这一设计，后续根据实际使用情况再进行调整

  关于 inode table 的处理感觉是可以更加灵活一些。在操作系统层面的处理，比如当我们想要为新创建的文件分配 inode 时，我们就需要从所有
可用 inode 中查找一个，此时新文件还不存在，所以也就没有文件锁，那此刻我们只能利用文件系统锁同步元数据；当我们已经明确知道某个文件存在
的时候，那我们就可以利用文件锁进行同步，因为该 inode 只对应于当前文件。除非执行删除后再进行操作，否则应该不太会涉及到文件系统锁

  实现过程中发现，文件系统 metadata 读取的情况还是很少的，基本上每次读都会对应写，最终还是要用写锁。索性改成互斥锁

  tptfs 加锁原则(个人理解，仅供参考)：
    1、处理 bitmap 使用文件系统锁
    2、处理超级块和块组，使用 tpt_mount 结构中的互斥锁
    3、尽量在靠上层的函数中加锁。感觉越往底层函数加锁，越不好管理
    4、inode table 分情况。如果是文件系统层级操作，例如从文件系统中找一个可用 inode，用文件系统锁；对已知文件的操作，用文件锁
    5、不同函数的使用场景会有差异，需要判定 VFileNode 实例是否已经存在，再决定是否使用文件锁


////////////// 20220301 //////////////
  TptItimesLocked 在 TptMkdir 函数中的调用情况有些特殊，此时 vnode 与 inode 已经关联，但是还没有创建对应的 VFileNode。所以
函数中贸然引用成员指针，就会导致段错误。更加说明，加锁还是要在偏上层的函数中添加，这样可以根据实际情况确定是采用文件锁还是文件系统锁

  inode table 的控制权会时常发生变化。比如当我们要创建一个新文件时，首先要向文件系统申请一个可用的 inode，此时 inode table 的
控制权是在文件系统手中。一旦分配完成，inode 就会对应到新文件，在实例化类对象之后，inode table 对应地址的控制权应该就要交给具体文件，
此时我们要修改或者读取数据的时候，就要用文件锁 (再用文件系统锁的话，会影响到其他创建新文件的进程，可能效率上会比较低)


////////////// 20220303 //////////////
  考虑删除文件的情形，此时可能已经将文件节点冲链表中移除，但是指向 inode 地址的指针还在。按照之前的设计，此时该地址区域的控制权应该已经
还给了文件系统，但是感觉好像又没有其他进程可以访问到该区域(文件节点已经删除了，应该也查找不到了)，所以还需不需要加锁？


////////////// 20220304 //////////////
  tptfs 中系统调用函数中添加读写锁，不太确定是否会产生互相嵌套，造成死锁？
  读优先或者写优先情况下，某些情形下可能会造成读写饥饿

  tmpfs 中 mount 结构或者节点里面的锁会标明它用于保护哪些数据，而且 tmpfs 应该也是会面临多线程访问同一个文件的使用情形，感觉可以参考
一下它的锁实现原理。可以对照 tmpfs 和 tptfs syscall 注册函数的数据同步机制上的差异，修改 tmpfs 的功能实现 

  tmpfs 读写数据函利用的是 uiomove_object 函数来实现的，其中用到的主要是宏 VM_OBJECT_WUNLOCK，也就是 vm_object-> lock 读写锁。
bread() 函数中主要是用到 struct bufobj->bo_lock 读写锁。所以，在 tptfs 中利用读写锁进行数据同步应该还是可以的。tmpfs 文件节点中
包含了一个 interlock 互斥锁，它用来保护文件节点中的时间，属性等字段，而不是像 tptfs 这样，利用文件的读写锁进行保护。所以，tptfs 可以
采用类似的方式，利用 fNodeLock 互斥锁保护时间，属性等等信息，这样做的好处就是将文件数据读写跟属性修改分开管理，不容易造成递归锁错误
  在上级函数执行加锁操作之后，本函数中不应该在使用 try_lock 或者 lock_owned 等函数在判断是否能够获取锁对象。如果出现了这种应用情形，
应该就要考虑函数逻辑是否存在问题，可以修改相应逻辑


  .bin 文件会根据链接脚本中实际的虚拟地址空间大小生成，没有数据的空间用0进行填充。elf 文件跟这个不一样，有些没有数据的段应该是折叠的，
所以实际文件并没有链接脚本中设置的那么大